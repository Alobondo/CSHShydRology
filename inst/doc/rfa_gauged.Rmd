---
title: "Prediction of flood quantiles at ungauged sites"
output: 
    rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{gauged-RFA}
  %\VignetteEngine{knitr::rmarkdown}
  \usepackage[utf8]{inputenc}
---

## Introduction

This document shows how to use `CSHShydRology` to perform regional 
flood frequency analysis using pooling groups and the index-flood model.
The methodology is presented using 45 hydrometric sites located in the Atlantic 
provinces of Canada. 
The dataset `flowAtlantic` contains the annual maximums of river discharge `ams`
and some catchment descriptors: drainage area, mean annual precipitation. 

```{r}
library(CSHShydRology)
data(flowAtlantic) 
ams <- flowAtlantic$ams
info <- flowAtlantic$info
```

## Measure of similarity

For this analysis, pooling groups are formed as the nearest sites of a target
site from which regional information will be transfered. 
Different measures of similarity exist to identify the members of a pooling
group.
Here we will used a measure of similarity based on the seasonality of the annual 
maximums that characterizes the flood regime of the site of interest.
This follow the recommendation of Mostofi Zadeh 
et al. (2019) that showed that the seasonality based on annual maximum discharges 
should be preferred for regional frequency analysis based threshold modeling.

An angular value (in radian) can be attributed to the moment when a flood is 
observed 
$$
\theta_i = \mathrm{(Julian \,date)}_i\left( \frac{2\pi}{365}\right).
$$
Accordingly, a sample of $n$ angular values is summarized by averaging each 
Cartesian coordinate separately.

$$
\begin{align}
\bar x &= \frac{1}{n}\sum_{i=1}^n \cos(\theta_i) \\
\bar y &= \frac{1}{n}\sum_{i=1}^n \sin(\theta_i) 
\end{align}
$$
Nevertheless, these summary statistics are easier to interpret in polar 
coordinates where $\theta$ represents the average moment of occurrence for the annual
maximum discharges and $r$ represent its regularity during the year.
In particular, values of $r$ close to 1 entails that the maximum discharge
occurs almost the same day every year, while values close to 0 imply that the 
maximum discharge could be observed at any moment of the years.

$$
\begin{align}
\theta &= \arctan \left( \bar y / \bar x\right)\\
r &= \sqrt{\bar x^2 + \bar y^2} \\
\end{align}
$$

The seasonality of one or several sites can be estimated using the `SeasonStat`
function.

```{r}
## Evaluate seasonal statistics
s <- ams[ams$id == ams$id[1], ]$date
SeasonStat(s)

st <- SeasonStat(date ~ id, ams)
head(st)
```

Next, the distance between sites can be evaluated using the Eucledean distance
between cartesian or polar coordinates. The function `DistSeason` compute the distance between pairs $(r_j,\theta_j)$ and
$(r_k, \theta_k)$ according to

$$
h_{i,j} =\sqrt{\|r_j-r_k \|^2 + \left(\frac{\|\theta_j-\theta_k\|}{\pi}\right)^2}
$$

```{r}
## Site names
sname <- as.character(info$id)

## Distance using cartesian coordinates
distance.st <- as.matrix(dist(st[,c('x','y')]))
colnames(distance.st) <- rownames(distance.st) <- sname

## Distance using polar coordinates
distance.st2 <- DistSeason(radius ~ angle , st)
colnames(distance.st2) <- rownames(distance.st2) <- sname

```


The graphic below shows the seasonal statistics of the Atlantic rivers. 
A k-means algorithm is used to divide the sites in three regions based on 
regularity and timing.


```{r, fig.height=5, fig.width=5}
set.seed(0)
km <- kmeans(as.dist(distance.st2), 3, nstart = 5)
regime <- c('blue','darkgreen','orange')[km$cluster]

JulianPlot()
points(st, pch = 16, col = regime)
```

## Pooling groups

The example below first used the function `DataWide`, to construct a matrix of 
the data where sites are represented by columns and years by rows.
Next, nearest sites to the target `01AK007` are extracted.
Here, the target is assumed to be the site with distance 0.
Notice that the example below makes sure that the columns of the distance matrix
respect the order imposed by the columns of `xd`.


```{r}

## Organisation of the data in a matrix
ams$year <- format(ams$date, '%Y')
xd <- DataWide(ams ~ id + year, ams)

## Extract a pooling groups
distance.target <- distance.st2[colnames(xd),'01AK007']
xd.target <- FindNearest(xd, distance = distance.target, 25)
```

## Index-flood model

The index-flood model is frequently used to evaluate flood quantile $Q_T$ 
of return period $T$ based on a regional growth curve that characterizes the
pooling group. 
This model assumes that all at-site distributions are proportional to the same
regional growth curve up to a scale factor $\mu$.
This scale factor is usually taken as the sample mean.
Therefore, the estimated flood quantile have the form
$$
Q_T = \mu \times q_T
$$
where $q_T$ is the flood quantile of the region growth curve.
Most of the time, the regional growth curve is estimated using the L-moment 
algorithm that takes that derived the regional parameter from the average of 
the sample L-moments of the sites in the pooling groups. 
A best distribution is then selected by identifying the distribution where the 
L-moments are the most coherent with the sample estimates.
See Burn (1990) and Hosking and Wallis (1997) for a more in depth
introduction.

The regional growth curve is estimated using the regional L-moments by the
function `FitRegLmom`.
The summary of the fitted model indicates that the best distribution is the
generalized extreme value (GEV) distribution.
This is determined by the Z-scores that identify the distribution with the 
closest L-kurtosis to its sample estimates.

```{r}
## Fit regional growth curve
fit.target <- FitRegLmom(xd.target)
print(fit.target)
```

A complementary tool to the Z-score is the L-moment ratio diagram 
that illustrates the L-skewness and the L-kurtosis of all sites.
It can be seen in the figure below that the average (red dot) is located near 
the theoretical line of the GEV.


```{r fig.height=5, fig.width=6}
plot(fit.target)
```

## Homogenous regions

To assess the hypothesis of the index-flood model, the degree of homogeneity of
the pooling group must be verified.
Accordingly, for a homogenous group each at-site L-coefficient of
variation $V$ or LCV should be close to their regional value.
Therefore, a large 
dispersion of the LCV provide evidences that the pooling groups is not homogenous. 
To this end, a common heterogeneity measure is
$$
H = \frac{V-\mu_V}{\sigma_V}
$$
where values $H < 1$ can be interpreted as acceptably homogenous, $1\leq H<2$ 
possibly heterogenous and $H \geq 2$ definitely heterogenous.

A pooling group may be found overall heterogeneous only because it contents few 
heterogenous elements. 
In turns, each site can be removed from the pooling group and the one that best
improve the heterogeneity measure $H$ should be permanently removed. 
This process can be repeated until a homogenous pooling group is found or that
a given stopping criteria is reached.
The function `PoolRemove` search for the heterogenous sites, removes them and 
updates the regional growth curve.
In the present situation, two sites are removed to obtain a homogenous pooling 
group.

```{r}
## Remove heterogenous sites 
fit.target2 <- PoolRemove(fit.target)

## New heterogeneity measure
fit.target2$stat[1]
```


## Intersite correlation

Inference for the L-moment algorithm require parametric bootstrap.
Snow accumulation or large rainfall system generate floods 
that affect multiple sites simultaneously, which creates spatial dependence 
among sites. 
This will result in more uncertainties about the estimated flood quantiles 
in comparison to independent data. 
To account for intersite correlation, a multivariate Normal distribution is used 
to simulate at spatially distributed data that are later transformed to 
the proper at-site distributions.

The spearman correlation $\rho_{i,j}$ between pairs of the i-th and j-th sites 
measure the degree of association between series of flood events.
This measure is well suited to quantify the intersite correlations between 
paired observations as it is invariant to the at-site distributions.
The relation
$$
\theta_{i,j} = 2 \sin\left(\rho_{i,j}\frac{\pi}{6}\right)
$$
links the correlation coefficients $\theta_{i,j}$ of the 
multivariate Normal distribution to the spearman correlation coefficients.
However, corrections should be done to ensure that the final covariance matrix
is positive definite (Higham, 2002).


The function `Intersite` can be used to estimate the parameter of the
multivariate normal distribution.
The output is a list containing among others the correlation coefficient `corr`
and their corrected value `model`.
The function `sitename` is used to extract the names of the sites in a 
pooling group.


```{r}
sid <- sitenames(fit.target2)
icor <- Intersite(xd[,sid])
round(icor$model[1:3,1:3],2)
```

The strength of the dependence may depend on the distance between sites. 
In this case, an exponential model can be fitted by the function `Intersite` 
if a distance is provided. 
The exponential model has the form

$$
\widehat{\theta}_{i,j} = \begin{cases}
(1-\tau) \exp\left(-3 \left|\frac{h}{\gamma}\right|^p \right) & h > 0  \\
1 & h = 0\\
\end{cases}
$$
where the nugget effect $\tau \in [0,1]$ that represents a discontinuity at the 
origin $h = 0$ and the range parameter $\gamma > 0$ that controls the decay of 
the correlation with respect to the distance are estimated by weighted least 
squares with weights proportional to the number of paired observations.
Note that it is important in the example below that the order of the column in the matrix of data and distance matched.


```{r fig.height=5, fig.width=6}

## Compute the distance
distance.geo <- GeoDist(~lon+lat, info)
colnames(distance.geo) <- rownames(distance.geo) <- sname
geo.target <- distance.geo[sid,sid]


## Fit the exponential model
icor2 <- Intersite(xd[,sid], method = 'exp',
                   distance = geo.target, 
                   distance.max = 300)

print(icor2)

## Display the results
theta <- icor2$corr[lower.tri(icor2$corr)]
theta.model <- icor2$model[lower.tri(icor2$corr)]
h <- geo.target[lower.tri(icor2$corr)]

plot(h, theta, pch = '.', cex = 2)
points(h, theta.model, col = 'red', pch = 16)

```


## Flood quantiles

Flood quantiles can be obtained from the function `predict`. 
The example below obtains the flood quantiles of 10 and 100 years return 
periods with different models of intersite correlations.
The argument `corr` is the correlation matrix of a multivariate Normal 
distribution used for the parametric bootstrap. 
A single value can also be passed, in which case a constant coefficient of 
correlation is used.
When, the function `Intersite` evaluates the correlation matrix 
empirically, it computes the average coefficient of correlation (`para`).

```{r}
## Using exponential model
predict(fit.target2, q = c(.9, .99), ci = TRUE, corr = icor2$model)

## Using constant coefficient of correlation
predict(fit.target2, q = c(.9, .99), ci = TRUE, corr = icor$para)
```

## Conclusion

In this document we showed how to perform a regional flood frequency analysis 
using similarity based seasonal statistics and pooling groups.
First seasonal statistics were evaluated (`SeasonStat`) as well as the
the pairwise distance (`DistSeason`) of the sites in the seasonal space.
The annual maximums were then reorganized in a matrix with sites in columns
and years in rows (`DataWide`).
Next, the nearest sites to a target were identified (`FindNearest`) and 
an index flood model was fitted (`FitRegLmom`) using the L-moments algorithm.
It was found that the pooling group of the target was not homogenous. 
The most heterogenous sites were then removed in a stepwise manner 
(`PoolRemove`) and the flood quantiles were finally estimated (`predict`).
To account for the intersite correlation that could lead to an underestimation
of the true uncertainty of the flood quantiles, a model for the 
intersite correlation was initially estimated (`Intersite`).

## References

* Burn, D. H. (1990). An appraisal of the “region of influence” approach to flood
  frequency analysis. Hydrological Sciences Journal, 35(2), 149–165.
  https://doi.org/10.1080/02626669009492415

* Burn, D. H. (1997). Catchment similarity for regional flood frequency analysis 
  using seasonality measures. Journal of Hydrology, 202(1–4), 212–230.
  https://doi.org/10.1016/S0022-1694(97)00068-1

* Durocher, M., Burn, D. H., & Mostofi Zadeh, S. (2018). A nationwide regional 
  flood frequency analysis at ungauged sites using ROI/GLS with copulas and 
  super regions. Journal of Hydrology, 567, 191–202. 
  https://doi.org/10.1016/j.jhydrol.2018.10.011


* Higham, Nick (2002). Computing the nearest correlation matrix - a problem 
  from finance; IMA Journal of Numerical Analysis 22, 329–343.
  https://doi.org/10.1093/imanum/22.3.329
  
* Hosking, J. R. M., & Wallis, J. R. (1997). Regional frequency analysis: an 
  approach based on L-moments. Cambridge Univ Pr.

* Mostofi Zadeh, S., Durocher, M., Burn, D. H., & Ashkar, F. (2019). Pooled flood 
  frequency analysis: a comparison based on peaks-over-threshold and annual 
  maximum series. Hydrological Sciences Journal, 0(ja), null.
  https://doi.org/10.1080/02626667.2019.1577556
